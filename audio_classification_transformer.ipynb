{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQlNIs0pi-hX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/meghana/dev/miniconda3/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/meghana/dev/miniconda3/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
      "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
      "  from numba.decorators import jit as optional_jit\n",
      "/Users/meghana/dev/miniconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Code from the next sources were used\n",
    "#[4] A transformer chatbot tutorial with Tensorflow 2.0. Retrieved from https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2\n",
    "#[6] https://www.kaggle.com/maxwell110/beginner-s-guide-to-audio-data-2\n",
    "#[7] https://www.kaggle.com/daisukelab/cnn-2d-basic-solution-powered-by-fast-ai\n",
    "\n",
    "# We load the requiere libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "# import git\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import zipfile\n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xUqQSW-NGIXF"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ebyq6pJaFXyT",
    "outputId": "470a1a84-98fe-4f4f-b58d-58b4336ec7c8"
   },
   "outputs": [],
   "source": [
    "# #We load csv with files name. We will use for reading the data\n",
    "# df = pd.read_csv('/home/facudeza/train_curated.csv')\n",
    "# #df_noisy = pd.read_csv('../input/freesound-audio-tagging-2019/train_noisy.csv')\n",
    "\n",
    "# #train=pd.concat([df, df_noisy], axis=0)\n",
    "\n",
    "# test_df = pd.read_csv('/home/facudeza//sample_submission.csv')\n",
    "\n",
    "# len(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'age_onset', 'birthplace', 'filename', 'native_language', 'sex',\n",
      "       'speakerid', 'country', 'file_missing?'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_onset</th>\n",
       "      <th>speakerid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2172.000000</td>\n",
       "      <td>2172.000000</td>\n",
       "      <td>2172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>33.117173</td>\n",
       "      <td>8.833333</td>\n",
       "      <td>1088.449355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.453039</td>\n",
       "      <td>8.451127</td>\n",
       "      <td>628.420329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>543.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1088.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1632.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>97.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>2176.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age    age_onset    speakerid\n",
       "count  2172.000000  2172.000000  2172.000000\n",
       "mean     33.117173     8.833333  1088.449355\n",
       "std      14.453039     8.451127   628.420329\n",
       "min       0.000000     0.000000     1.000000\n",
       "25%      22.000000     0.000000   543.750000\n",
       "50%      28.000000     8.000000  1088.500000\n",
       "75%      41.000000    13.000000  1632.250000\n",
       "max      97.000000    86.000000  2176.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "import IPython.display as ipd  # To play sound in the notebook\n",
    "import librosa\n",
    "import librosa.display\n",
    "import time\n",
    "import warnings\n",
    "import pickle as pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "SAMPLE_RATE = 22050\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv(\"speakers_all.csv\")\n",
    "df.drop(df.columns[9:12],axis = 1, inplace = True)\n",
    "print(df.columns)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "time taken 0.9447706699371338\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# def extract_features_from_pkl(fname):\n",
    "#     try:\n",
    "#         path = 'features/' + fname +\".pkl\"\n",
    "#         extracted_data = pd.read_pickle(path)\n",
    "#         data, sampling_rate = extracted_data[\"data\"], extracted_data[\"sampling_rate\"]\n",
    "#         duration = extracted_data[\"duration\"]\n",
    "#         mfcc = extracted_data[\"mfcc\"]\n",
    "#         melspec = extracted_data[\"melspec\"]\n",
    "#         y_harmonic, y_percussive = extracted_data[\"y_harmonic\"],extracted_data[\"y_percussive\"]\n",
    "#         melspec_h = extracted_data[\"melspec_h\"]\n",
    "#         log_h = extracted_data[\"log_h\"]\n",
    "#         melspec_p = extracted_data[\"melspec_p\"]\n",
    "#         log_p = extracted_data[\"log_p\"]\n",
    "#         zero_crossing_rate = extracted_data[\"zero_crossing_rate\"]\n",
    "#         spectral_rolloff = extracted_data[\"spectral_rolloff\"]\n",
    "#         spectral_centroid = extracted_data[\"spectral_centroid\"]\n",
    "#         spectral_contrast = extracted_data[\"spectral_contrast\"]\n",
    "#         spectral_bandwidth = extracted_data[\"spectral_bandwidth\"]\n",
    "#         chroma_cqt = extracted_data[\"chroma_cqt\"]\n",
    "#         return (duration,mfcc,melspec,melspec_h,melspec_p,y_harmonic, y_percussive,log_h\n",
    "#                 ,log_p,data,sampling_rate,zero_crossing_rate,spectral_rolloff,spectral_centroid\n",
    "#                 ,spectral_contrast,spectral_bandwidth,chroma_cqt)\n",
    "#     except Exception:\n",
    "#         return \n",
    "\n",
    "# durations,mfccs,melspecs,melspec_hs,melspec_ps,y_harmonics,y_percussives = [],[],[],[],[],[],[]\n",
    "# log_hs,log_ps,datas,sampling_rates = [],[],[],[]\n",
    "# zero_crossing_rates,spectral_rolloffs, spectral_centroids = [],[],[]\n",
    "# spectral_contrasts,spectral_bandwidths,chroma_cqts = [],[],[]\n",
    "# for _, row in df.iterrows():\n",
    "#     print(_)\n",
    "#     filename = row['filename']\n",
    "#     val = extract_features_from_pkl(filename)\n",
    "#     if val:\n",
    "#         durations.append(val[0])\n",
    "#         mfccs.append(val[1])\n",
    "#         melspecs.append(val[2])\n",
    "#         melspec_hs.append(val[3])\n",
    "#         melspec_ps.append(val[4])\n",
    "#         y_harmonics.append(val[5])\n",
    "#         y_percussives.append(val[6])\n",
    "#         log_hs.append(val[7])\n",
    "#         log_ps.append(val[8])\n",
    "#         datas.append(val[9])\n",
    "#         sampling_rates.append(val[10])\n",
    "#         zero_crossing_rates.append(val[11])\n",
    "#         spectral_rolloffs.append(val[12])\n",
    "#         spectral_centroids.append(val[13])\n",
    "#         spectral_contrasts.append(val[14])\n",
    "#         spectral_bandwidths.append(val[15])\n",
    "#         chroma_cqts.append(val[16])\n",
    "#     else:\n",
    "#         durations.append(None)\n",
    "#         mfccs.append(None)\n",
    "#         melspecs.append(None)\n",
    "#         melspec_hs.append(None)\n",
    "#         melspec_ps.append(None)\n",
    "#         y_harmonics.append(None)\n",
    "#         y_percussives.append(None)\n",
    "#         log_hs.append(None)\n",
    "#         log_ps.append(None)\n",
    "#         datas.append(None)\n",
    "#         sampling_rates.append(None)\n",
    "#         zero_crossing_rates.append(None)\n",
    "#         spectral_rolloffs.append(None)\n",
    "#         spectral_centroids.append(None)\n",
    "#         spectral_contrasts.append(None)\n",
    "#         spectral_bandwidths.append(None)\n",
    "#         chroma_cqts.append(None)\n",
    "\n",
    "# print(\"time taken \"+str((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>native_language</th>\n",
       "      <th>sex</th>\n",
       "      <th>country</th>\n",
       "      <th>duration</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>melspec</th>\n",
       "      <th>melspec_h</th>\n",
       "      <th>melspec_p</th>\n",
       "      <th>...</th>\n",
       "      <th>melspec_harmonic</th>\n",
       "      <th>melspec_percussive</th>\n",
       "      <th>librosa_data</th>\n",
       "      <th>librosa_sampling_rate</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>spectral_rolloff</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_contrast</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>chroma_cqt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>27.0</td>\n",
       "      <td>virginia, south africa</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>female</td>\n",
       "      <td>south africa</td>\n",
       "      <td>20.772290</td>\n",
       "      <td>[[-395.00208, -387.00363, -377.9013, -392.9432...</td>\n",
       "      <td>[[0.09874383, 0.25136992, 0.25506964, 0.223414...</td>\n",
       "      <td>[[0.048649844, 0.11224768, 0.10517518, 0.06032...</td>\n",
       "      <td>[[0.016398141, 0.033163264, 0.038705193, 0.063...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-5.995369, -5.995369, -5.995369, -5.995369, ...</td>\n",
       "      <td>[[-30.363499, -29.586853, -28.244614, -23.9563...</td>\n",
       "      <td>[0.0071707093, 0.010596839, 0.00808429, 0.0093...</td>\n",
       "      <td>22050.0</td>\n",
       "      <td>[0.01611328125, 0.0244140625, 0.033203125, 0.0...</td>\n",
       "      <td>[2950.048828125, 2971.58203125, 3068.481445312...</td>\n",
       "      <td>[1530.4223975774694, 1423.8069150707834, 1320....</td>\n",
       "      <td>[36.50567083862842, 14.012752078187578, 10.362...</td>\n",
       "      <td>[2463.8000385430314, 2273.238307315652, 2083.8...</td>\n",
       "      <td>[[0.8212306011679301, 0.8040046693991846, 0.78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40.0</td>\n",
       "      <td>pretoria, south africa</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>21.961497</td>\n",
       "      <td>[[-560.3812, -560.3812, -560.3812, -560.3812, ...</td>\n",
       "      <td>[[2.1711987e-06, 2.455684e-06, 2.3796056e-06, ...</td>\n",
       "      <td>[[2.1903074e-06, 2.4361193e-06, 2.391069e-06, ...</td>\n",
       "      <td>[[4.3559062e-10, 2.1939425e-10, 1.1125337e-09,...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-24.149624, -24.149624, -24.149624, -24.1496...</td>\n",
       "      <td>[[-24.416653, -24.416653, -24.416653, -24.4166...</td>\n",
       "      <td>[-1.8169361e-05, -2.4786286e-05, -8.806779e-06...</td>\n",
       "      <td>22050.0</td>\n",
       "      <td>[0.025390625, 0.041015625, 0.05029296875, 0.04...</td>\n",
       "      <td>[8990.1123046875, 9108.544921875, 9248.5107421...</td>\n",
       "      <td>[5123.089311931954, 5264.551708269, 5429.44865...</td>\n",
       "      <td>[32.56676585595034, 28.16572518059867, 24.7029...</td>\n",
       "      <td>[3348.660970144079, 3348.077664928403, 3389.43...</td>\n",
       "      <td>[[0.9168646531128495, 0.6017439302030426, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>43.0</td>\n",
       "      <td>pretoria, transvaal, south africa</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>26.880000</td>\n",
       "      <td>[[-441.16895, -439.80875, -430.03815, -406.936...</td>\n",
       "      <td>[[0.006251498, 0.005871368, 0.0032841666, 0.00...</td>\n",
       "      <td>[[0.00229122, 0.0017953095, 0.0010293978, 0.00...</td>\n",
       "      <td>[[0.0010243187, 0.0012062755, 0.00065918657, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-17.287262, -17.287262, -17.287262, -17.2872...</td>\n",
       "      <td>[[-15.318008, -15.318008, -15.318008, -15.3180...</td>\n",
       "      <td>[0.0019851814, 0.0027917789, 0.0013547958, 0.0...</td>\n",
       "      <td>22050.0</td>\n",
       "      <td>[0.0244140625, 0.0322265625, 0.05126953125, 0....</td>\n",
       "      <td>[3079.248046875, 2627.05078125, 2616.284179687...</td>\n",
       "      <td>[1465.0935000653856, 1361.8608658509977, 1347....</td>\n",
       "      <td>[22.474867417512268, 26.549838842792123, 29.65...</td>\n",
       "      <td>[2159.5564308533862, 2036.7410293218215, 1875....</td>\n",
       "      <td>[[0.9405656123995829, 0.8434395912814198, 0.93...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>26.0</td>\n",
       "      <td>pretoria, south africa</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>23.471202</td>\n",
       "      <td>[[-529.75555, -529.76184, -529.74896, -529.790...</td>\n",
       "      <td>[[0.0005168101, 0.0005084191, 0.00052578124, 0...</td>\n",
       "      <td>[[0.00051670696, 0.0005085286, 0.00052552565, ...</td>\n",
       "      <td>[[5.3004323e-11, 8.718523e-11, 3.5090522e-10, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-18.92363, -18.92363, -18.92363, -18.92363, ...</td>\n",
       "      <td>[[-17.007862, -17.007862, -17.007862, -17.0078...</td>\n",
       "      <td>[0.00025531906, 0.0003802524, 0.00032837203, 0...</td>\n",
       "      <td>22050.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[8182.6171875, 8322.5830078125, 8268.75, 7967....</td>\n",
       "      <td>[2836.1443828141787, 2981.4239632758927, 2943....</td>\n",
       "      <td>[31.146817865859134, 33.83682462665064, 35.432...</td>\n",
       "      <td>[3677.497426953188, 3705.7593851497895, 3685.5...</td>\n",
       "      <td>[[0.43540816224779805, 0.43058722215309736, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>19.0</td>\n",
       "      <td>cape town, south africa</td>\n",
       "      <td>afrikaans</td>\n",
       "      <td>male</td>\n",
       "      <td>south africa</td>\n",
       "      <td>20.252426</td>\n",
       "      <td>[[-509.4199, -479.27243, -467.60425, -471.0367...</td>\n",
       "      <td>[[0.595527, 2.648835, 4.051913, 2.793845, 0.93...</td>\n",
       "      <td>[[0.40962306, 1.6456182, 3.0860555, 2.5644944,...</td>\n",
       "      <td>[[0.029812008, 0.13252573, 0.09091881, 0.00693...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[-7.7523117, 4.3265815, 9.7880745, 8.180036, ...</td>\n",
       "      <td>[[-30.512177, -17.553995, -20.826927, -35.1874...</td>\n",
       "      <td>[1.7647135e-05, -1.7076701e-06, -9.886354e-06,...</td>\n",
       "      <td>22050.0</td>\n",
       "      <td>[0.0126953125, 0.01513671875, 0.0166015625, 0....</td>\n",
       "      <td>[1668.8232421875, 1604.2236328125, 1701.123046...</td>\n",
       "      <td>[808.4449193168485, 717.8543169790694, 817.630...</td>\n",
       "      <td>[20.726607494551942, 19.790129878017588, 21.93...</td>\n",
       "      <td>[1641.2152764813704, 1536.646836041162, 1601.9...</td>\n",
       "      <td>[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age                         birthplace native_language     sex  \\\n",
       "32  27.0             virginia, south africa       afrikaans  female   \n",
       "33  40.0             pretoria, south africa       afrikaans    male   \n",
       "34  43.0  pretoria, transvaal, south africa       afrikaans    male   \n",
       "35  26.0             pretoria, south africa       afrikaans    male   \n",
       "36  19.0            cape town, south africa       afrikaans    male   \n",
       "\n",
       "         country   duration  \\\n",
       "32  south africa  20.772290   \n",
       "33  south africa  21.961497   \n",
       "34  south africa  26.880000   \n",
       "35  south africa  23.471202   \n",
       "36  south africa  20.252426   \n",
       "\n",
       "                                                 mfcc  \\\n",
       "32  [[-395.00208, -387.00363, -377.9013, -392.9432...   \n",
       "33  [[-560.3812, -560.3812, -560.3812, -560.3812, ...   \n",
       "34  [[-441.16895, -439.80875, -430.03815, -406.936...   \n",
       "35  [[-529.75555, -529.76184, -529.74896, -529.790...   \n",
       "36  [[-509.4199, -479.27243, -467.60425, -471.0367...   \n",
       "\n",
       "                                              melspec  \\\n",
       "32  [[0.09874383, 0.25136992, 0.25506964, 0.223414...   \n",
       "33  [[2.1711987e-06, 2.455684e-06, 2.3796056e-06, ...   \n",
       "34  [[0.006251498, 0.005871368, 0.0032841666, 0.00...   \n",
       "35  [[0.0005168101, 0.0005084191, 0.00052578124, 0...   \n",
       "36  [[0.595527, 2.648835, 4.051913, 2.793845, 0.93...   \n",
       "\n",
       "                                            melspec_h  \\\n",
       "32  [[0.048649844, 0.11224768, 0.10517518, 0.06032...   \n",
       "33  [[2.1903074e-06, 2.4361193e-06, 2.391069e-06, ...   \n",
       "34  [[0.00229122, 0.0017953095, 0.0010293978, 0.00...   \n",
       "35  [[0.00051670696, 0.0005085286, 0.00052552565, ...   \n",
       "36  [[0.40962306, 1.6456182, 3.0860555, 2.5644944,...   \n",
       "\n",
       "                                            melspec_p  ...  \\\n",
       "32  [[0.016398141, 0.033163264, 0.038705193, 0.063...  ...   \n",
       "33  [[4.3559062e-10, 2.1939425e-10, 1.1125337e-09,...  ...   \n",
       "34  [[0.0010243187, 0.0012062755, 0.00065918657, 0...  ...   \n",
       "35  [[5.3004323e-11, 8.718523e-11, 3.5090522e-10, ...  ...   \n",
       "36  [[0.029812008, 0.13252573, 0.09091881, 0.00693...  ...   \n",
       "\n",
       "                                     melspec_harmonic  \\\n",
       "32  [[-5.995369, -5.995369, -5.995369, -5.995369, ...   \n",
       "33  [[-24.149624, -24.149624, -24.149624, -24.1496...   \n",
       "34  [[-17.287262, -17.287262, -17.287262, -17.2872...   \n",
       "35  [[-18.92363, -18.92363, -18.92363, -18.92363, ...   \n",
       "36  [[-7.7523117, 4.3265815, 9.7880745, 8.180036, ...   \n",
       "\n",
       "                                   melspec_percussive  \\\n",
       "32  [[-30.363499, -29.586853, -28.244614, -23.9563...   \n",
       "33  [[-24.416653, -24.416653, -24.416653, -24.4166...   \n",
       "34  [[-15.318008, -15.318008, -15.318008, -15.3180...   \n",
       "35  [[-17.007862, -17.007862, -17.007862, -17.0078...   \n",
       "36  [[-30.512177, -17.553995, -20.826927, -35.1874...   \n",
       "\n",
       "                                         librosa_data librosa_sampling_rate  \\\n",
       "32  [0.0071707093, 0.010596839, 0.00808429, 0.0093...               22050.0   \n",
       "33  [-1.8169361e-05, -2.4786286e-05, -8.806779e-06...               22050.0   \n",
       "34  [0.0019851814, 0.0027917789, 0.0013547958, 0.0...               22050.0   \n",
       "35  [0.00025531906, 0.0003802524, 0.00032837203, 0...               22050.0   \n",
       "36  [1.7647135e-05, -1.7076701e-06, -9.886354e-06,...               22050.0   \n",
       "\n",
       "                                   zero_crossing_rate  \\\n",
       "32  [0.01611328125, 0.0244140625, 0.033203125, 0.0...   \n",
       "33  [0.025390625, 0.041015625, 0.05029296875, 0.04...   \n",
       "34  [0.0244140625, 0.0322265625, 0.05126953125, 0....   \n",
       "35  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "36  [0.0126953125, 0.01513671875, 0.0166015625, 0....   \n",
       "\n",
       "                                     spectral_rolloff  \\\n",
       "32  [2950.048828125, 2971.58203125, 3068.481445312...   \n",
       "33  [8990.1123046875, 9108.544921875, 9248.5107421...   \n",
       "34  [3079.248046875, 2627.05078125, 2616.284179687...   \n",
       "35  [8182.6171875, 8322.5830078125, 8268.75, 7967....   \n",
       "36  [1668.8232421875, 1604.2236328125, 1701.123046...   \n",
       "\n",
       "                                    spectral_centroid  \\\n",
       "32  [1530.4223975774694, 1423.8069150707834, 1320....   \n",
       "33  [5123.089311931954, 5264.551708269, 5429.44865...   \n",
       "34  [1465.0935000653856, 1361.8608658509977, 1347....   \n",
       "35  [2836.1443828141787, 2981.4239632758927, 2943....   \n",
       "36  [808.4449193168485, 717.8543169790694, 817.630...   \n",
       "\n",
       "                                    spectral_contrast  \\\n",
       "32  [36.50567083862842, 14.012752078187578, 10.362...   \n",
       "33  [32.56676585595034, 28.16572518059867, 24.7029...   \n",
       "34  [22.474867417512268, 26.549838842792123, 29.65...   \n",
       "35  [31.146817865859134, 33.83682462665064, 35.432...   \n",
       "36  [20.726607494551942, 19.790129878017588, 21.93...   \n",
       "\n",
       "                                   spectral_bandwidth  \\\n",
       "32  [2463.8000385430314, 2273.238307315652, 2083.8...   \n",
       "33  [3348.660970144079, 3348.077664928403, 3389.43...   \n",
       "34  [2159.5564308533862, 2036.7410293218215, 1875....   \n",
       "35  [3677.497426953188, 3705.7593851497895, 3685.5...   \n",
       "36  [1641.2152764813704, 1536.646836041162, 1601.9...   \n",
       "\n",
       "                                           chroma_cqt  \n",
       "32  [[0.8212306011679301, 0.8040046693991846, 0.78...  \n",
       "33  [[0.9168646531128495, 0.6017439302030426, 1.0,...  \n",
       "34  [[0.9405656123995829, 0.8434395912814198, 0.93...  \n",
       "35  [[0.43540816224779805, 0.43058722215309736, 0....  \n",
       "36  [[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"duration\"] = durations\n",
    "# df[\"mfcc\"] = mfccs\n",
    "# df[\"melspec\"] = melspecs\n",
    "# df[\"melspec_h\"] = melspec_hs\n",
    "# df[\"melspec_p\"] = melspec_ps\n",
    "# df[\"y_harmonic\"] = y_harmonics\n",
    "# df[\"y_percussive\"] = y_percussives\n",
    "# df[\"melspec_harmonic\"] = log_hs\n",
    "# df[\"melspec_percussive\"] = log_ps\n",
    "# df[\"librosa_data\"] = datas\n",
    "# df[\"librosa_sampling_rate\"] = sampling_rates\n",
    "# df[\"zero_crossing_rate\"] = zero_crossing_rates\n",
    "# df[\"spectral_rolloff\"] = spectral_rolloffs\n",
    "# df[\"spectral_centroid\"] = spectral_centroids\n",
    "# df[\"spectral_contrast\"] = spectral_contrasts\n",
    "# df[\"spectral_bandwidth\"] = spectral_bandwidths\n",
    "# df[\"chroma_cqt\"] = chroma_cqts\n",
    "# df = df.dropna()\n",
    "# df.head()\n",
    "# df.drop(columns=['age_onset','filename','speakerid','file_missing?'],inplace=True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "time taken 0.39409873485565183\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def extract_mfcc_melspecs_from_pkl(fname):\n",
    "    try:\n",
    "        path = 'features/' + fname +\".pkl\"\n",
    "        extracted_data = pd.read_pickle(path)\n",
    "        data, sampling_rate = extracted_data[\"data\"], extracted_data[\"sampling_rate\"]\n",
    "        duration = extracted_data[\"duration\"]\n",
    "        mfcc = extracted_data[\"mfcc\"]\n",
    "        melspec = extracted_data[\"melspec\"]\n",
    "        return (duration,mfcc,melspec,data)\n",
    "    except Exception:\n",
    "        return \n",
    "\n",
    "durations,mfccs,melspecs,datas = [],[],[],[]\n",
    "for _, row in df.iterrows():\n",
    "    print(_)\n",
    "    filename = row['filename']\n",
    "    val = extract_mfcc_melspecs_from_pkl(filename)\n",
    "    if val:\n",
    "        durations.append(val[0])\n",
    "        mfccs.append(val[1])\n",
    "        melspecs.append(val[2])\n",
    "        datas.append(val[3])\n",
    "    else:\n",
    "        durations.append(None)\n",
    "        mfccs.append(None)\n",
    "        melspecs.append(None)\n",
    "        datas.append(None)\n",
    "\n",
    "df[\"duration\"] = durations\n",
    "df[\"mfcc\"] = mfccs\n",
    "df[\"melspec\"] = melspecs\n",
    "df[\"librosa_data\"] = datas\n",
    "\n",
    "\n",
    "print(\"time taken \"+str((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_onset</th>\n",
       "      <th>birthplace</th>\n",
       "      <th>filename</th>\n",
       "      <th>native_language</th>\n",
       "      <th>sex</th>\n",
       "      <th>speakerid</th>\n",
       "      <th>country</th>\n",
       "      <th>file_missing?</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>wolof</th>\n",
       "      <th>wu</th>\n",
       "      <th>xasonga</th>\n",
       "      <th>xiang</th>\n",
       "      <th>yakut</th>\n",
       "      <th>yapese</th>\n",
       "      <th>yiddish</th>\n",
       "      <th>yoruba</th>\n",
       "      <th>yupik</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>46.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>lagos, nigeria</td>\n",
       "      <td>yoruba3</td>\n",
       "      <td>yoruba</td>\n",
       "      <td>female</td>\n",
       "      <td>766</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>46.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>lagos, nigeria</td>\n",
       "      <td>yoruba4</td>\n",
       "      <td>yoruba</td>\n",
       "      <td>male</td>\n",
       "      <td>851</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>ibadan, nigeria</td>\n",
       "      <td>yoruba5</td>\n",
       "      <td>yoruba</td>\n",
       "      <td>female</td>\n",
       "      <td>2023</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bethel, alaska, usa</td>\n",
       "      <td>yupik1</td>\n",
       "      <td>yupik</td>\n",
       "      <td>female</td>\n",
       "      <td>571</td>\n",
       "      <td>usa</td>\n",
       "      <td>False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>24.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>bulawayo, zimbabwe</td>\n",
       "      <td>zulu1</td>\n",
       "      <td>zulu</td>\n",
       "      <td>female</td>\n",
       "      <td>406</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>False</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  228 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  age_onset           birthplace filename native_language     sex  \\\n",
       "2167  46.0        5.0       lagos, nigeria  yoruba3          yoruba  female   \n",
       "2168  46.0       12.0       lagos, nigeria  yoruba4          yoruba    male   \n",
       "2169  47.0        2.0      ibadan, nigeria  yoruba5          yoruba  female   \n",
       "2170  31.0        1.0  bethel, alaska, usa   yupik1           yupik  female   \n",
       "2171  24.0       14.0   bulawayo, zimbabwe    zulu1            zulu  female   \n",
       "\n",
       "      speakerid   country  file_missing?  duration  ... wolof   wu xasonga  \\\n",
       "2167        766   nigeria          False      15.0  ...   0.0  0.0     0.0   \n",
       "2168        851   nigeria          False      15.0  ...   0.0  0.0     0.0   \n",
       "2169       2023   nigeria          False      15.0  ...   0.0  0.0     0.0   \n",
       "2170        571       usa          False      15.0  ...   0.0  0.0     0.0   \n",
       "2171        406  zimbabwe          False      15.0  ...   0.0  0.0     0.0   \n",
       "\n",
       "     xiang  yakut  yapese  yiddish  yoruba  yupik  zulu  \n",
       "2167   0.0    0.0     0.0      0.0     1.0    0.0   0.0  \n",
       "2168   0.0    0.0     0.0      0.0     1.0    0.0   0.0  \n",
       "2169   0.0    0.0     0.0      0.0     1.0    0.0   0.0  \n",
       "2170   0.0    0.0     0.0      0.0     0.0    1.0   0.0  \n",
       "2171   0.0    0.0     0.0      0.0     0.0    0.0   1.0  \n",
       "\n",
       "[5 rows x 228 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = np.sort(np.unique(df['native_language'].values))\n",
    "num_class = len(labels)\n",
    "c2i = {}\n",
    "i2c = {}\n",
    "for i, c in enumerate(labels):\n",
    "    c2i[c] = i\n",
    "    i2c[i] = c\n",
    "\n",
    "\n",
    "def test(index):\n",
    "    global num_class\n",
    "    labels_array = np.zeros((num_class))\n",
    "    labels_array[index] = 1\n",
    "    return labels_array\n",
    "\n",
    "one_hot_labels = [test(c2i[x]) for x in df['native_language'].values]\n",
    "df[\"native_lang_c2i\"] = one_hot_labels\n",
    "df[\"native_lang_c2i\"].head()\n",
    "for f in labels:\n",
    "    df[f] = 0.0\n",
    "    #train_noisy[f] = 0.0\n",
    "\n",
    "df[labels] = one_hot_labels\n",
    "df.tail()\n",
    "\n",
    "# def split_and_label(rows_labels):\n",
    "    \n",
    "#     row_labels_list = []\n",
    "#     for row in rows_labels:\n",
    "#         row_labels = row.split(',')\n",
    "#         labels_array = np.zeros((80))\n",
    "        \n",
    "#         for label in row_labels:\n",
    "#             index = label_mapping[label]\n",
    "#             labels_array[index] = 1\n",
    "        \n",
    "#         row_labels_list.append(labels_array)\n",
    "    \n",
    "#     return row_labels_list\n",
    "\n",
    "\n",
    "# train_curated_labels = split_and_label(df['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu1s4aUVFXyX"
   },
   "outputs": [],
   "source": [
    "# We get the labels from the test data, we start from 1 because 0 is the file name\n",
    "# Also, test set only has 80 classes and want to predict according to that.\n",
    "# Of course, you could create \n",
    "# From here we will create Y target\n",
    "\n",
    "# label_columns = test_df.columns[1:]\n",
    "# label_mapping = dict((label, index) for index, label in enumerate(label_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTmsBU7CKu7p",
    "outputId": "911c334c-9fe4-4a3d-9962-a78de1de940f"
   },
   "outputs": [],
   "source": [
    "## We can see that several targets are in same space. We need to reshape it into a hot encode format\n",
    "# print(df['labels'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5K-aYlWFXyr"
   },
   "outputs": [],
   "source": [
    "# def split_and_label(rows_labels):\n",
    "    \n",
    "#     row_labels_list = []\n",
    "#     for row in rows_labels:\n",
    "#         row_labels = row.split(',')\n",
    "#         labels_array = np.zeros((80))\n",
    "        \n",
    "#         for label in row_labels:\n",
    "#             index = label_mapping[label]\n",
    "#             labels_array[index] = 1\n",
    "        \n",
    "#         row_labels_list.append(labels_array)\n",
    "    \n",
    "#     return row_labels_list\n",
    "\n",
    "\n",
    "# train_curated_labels = split_and_label(df['labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_HMtSma0i-h1",
    "outputId": "cdcf1f8d-cf12-42ab-abe3-f804bd08252b"
   },
   "outputs": [],
   "source": [
    "# for f in label_columns:\n",
    "#     df[f] = 0.0\n",
    "#     #train_noisy[f] = 0.0\n",
    "\n",
    "# df[label_columns] = train_curated_labels\n",
    "# #train_noisy[label_columns]   = train_noisy_labels\n",
    "\n",
    "# df['num_labels'] = df[label_columns].sum(axis=1)\n",
    "# #train_noisy['num_labels']   = train_noisy[label_columns].sum(axis=1)\n",
    "\n",
    "# #train_curated['path'] = '../input/train_curated/'+train_curated['fname']\n",
    "# #train_noisy  ['path'] = '../input/train_noisy/'+train_noisy['fname']\n",
    "\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wigpoc-oFXyf"
   },
   "outputs": [],
   "source": [
    "# unzipping our files\n",
    "\n",
    "\n",
    "# with zipfile.ZipFile('/home/facudeza/train_curated.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/home/jupyter/Untitled Folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJ0DebAQFXyo"
   },
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#with zipfile.ZipFile('../input/freesound-audio-tagging-2019/train_noisy.zip', 'r') as zip_ref:\n",
    " #   zip_ref.extractall('../output/kaggle/working/train')\n",
    "\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_mJMOENuFXyk"
   },
   "outputs": [],
   "source": [
    "## We define this parameters for our audio processing\n",
    "\n",
    "class conf:\n",
    "    # Preprocessing settings\n",
    "    sampling_rate = 44100\n",
    "    duration = 4\n",
    "    hop_length = 300\n",
    "    fmin = 20\n",
    "    fmax = sampling_rate // 2\n",
    "    n_mels = 128\n",
    "    n_fft = n_mels * 20\n",
    "    samples = sampling_rate * duration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eneZDekCNtNF"
   },
   "outputs": [],
   "source": [
    "# train_path = '/home/jupyter/Untitled Folder'\n",
    "#train_noisy_path = '../input/train_noisy/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_6O_hIOFXzX"
   },
   "outputs": [],
   "source": [
    "## We read the audio data, and trim or padded, if it is needed\n",
    "## This code was borrowed from https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold\n",
    "\n",
    "# def read_audio(conf, pathname, trim_long_data):\n",
    "#     y, sr = librosa.load(pathname, sr=conf.sampling_rate)\n",
    "#     # trim silence\n",
    "#     if 0 < len(y): # workaround: 0 length causes error\n",
    "#         y, _ = librosa.effects.trim(y) # trim, top_db=default(60)\n",
    "#     # make it unified length to conf.samples\n",
    "#     if len(y) > conf.samples: # long enough\n",
    "#         if trim_long_data:\n",
    "#             y = y[0:0+conf.samples]\n",
    "#     else: # pad blank\n",
    "#         padding = conf.samples - len(y)    # add padding at both ends\n",
    "#         offset = padding // 2\n",
    "#         y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n",
    "#     return y\n",
    "\n",
    "\n",
    "# ## We convert our raw audio into a mel spectogram\n",
    "# def audio_to_melspectrogram(conf, audio):\n",
    "#     spectrogram = librosa.feature.melspectrogram(audio, \n",
    "#                                                  sr=conf.sampling_rate,\n",
    "#                                                  n_mels=conf.n_mels,\n",
    "#                                                  hop_length=conf.hop_length,\n",
    "#                                                  n_fft=conf.n_fft,\n",
    "#                                                  fmin=conf.fmin,\n",
    "#                                                  fmax=conf.fmax)\n",
    "#     spectrogram = librosa.power_to_db(spectrogram)\n",
    "#     spectrogram = spectrogram.astype(np.float32)\n",
    "#     return spectrogram\n",
    "\n",
    "# #This implement the previous two function for one file\n",
    "# def read_as_melspectrogram(conf, pathname, trim_long_data, debug_display=False):\n",
    "#     x = read_audio(conf, pathname, trim_long_data)\n",
    "#     mels = audio_to_melspectrogram(conf, x)\n",
    "#     if debug_display:\n",
    "#         IPython.display.display(IPython.display.Audio(x, rate=conf.sampling_rate))\n",
    "#         show_melspectrogram(conf, mels)\n",
    "#     return mels\n",
    "\n",
    "# #This generalizes for all the audio files you have\n",
    "# def convert_wav_to_image(df, source):\n",
    "#     X = []\n",
    "#     for i, row in tqdm_notebook(df.iterrows()):\n",
    "#         try:\n",
    "#             x = read_as_melspectrogram(conf, source[0]+'/'+str(row.fname), trim_long_data=True)\n",
    "#         except:\n",
    "#             x = read_as_melspectrogram(conf, source[1]+'/'+str(row.fname), trim_long_data=True)\n",
    "\n",
    "#         X.append(x.transpose())\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 646\n"
     ]
    }
   ],
   "source": [
    "xmin,ymin = 128,float(\"inf\")\n",
    "for _,row in df.iterrows():\n",
    "    x,y = row[\"melspec\"].shape\n",
    "    xmin,ymin = min(x,xmin),min(y,ymin)\n",
    "print(xmin,ymin)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(964514,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"librosa_data\"].iloc[32].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 646)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"melspec\"].iloc[32][:,:680].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trim_data(y,trim_long_data):\n",
    "#     print(len(y),conf.samples)\n",
    "#     if len(y) > conf.samples: # long enough\n",
    "#         print(\"if\")\n",
    "#         if trim_long_data:\n",
    "#             y = y[0:0+conf.samples]\n",
    "#     else: # pad blank\n",
    "#         print(\"else\")\n",
    "#         padding = conf.samples - len(y)    # add padding at both ends\n",
    "#         offset = padding // 2\n",
    "#         y = np.pad(y, (offset, conf.samples - len(y) - offset), 'constant')\n",
    "#     print(y)\n",
    "#     return y\n",
    "\n",
    "\n",
    "\n",
    "def get_melspec(df):\n",
    "    X = []\n",
    "    for i, row in df.iterrows():\n",
    "        x = row[\"melspec\"]\n",
    "#         x = x[:,:680]\n",
    "        spectrogram = librosa.power_to_db(x)\n",
    "        spectrogram = spectrogram.astype(np.float32)\n",
    "        X.append(x.transpose())\n",
    "    return X\n",
    "\n",
    "X = np.array(get_melspec(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2133, 646, 128)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b8f8bc2431744d809fed238c4f8ef542",
      "991302236e84415ba1b4d067ae49c70b"
     ]
    },
    "colab_type": "code",
    "id": "4vAdpu4BFXza",
    "outputId": "7da8f902-8335-4a9e-a9b2-339d4b91363d"
   },
   "outputs": [],
   "source": [
    "#We get all our spectrograms\n",
    "# X = np.array(convert_wav_to_image(df, source=[train_path]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qX7R-Rtgi-iR"
   },
   "outputs": [],
   "source": [
    "#np.savez('/home/jupyter/curated_subset.npz',X=X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXdnwT_Ei-iU",
    "outputId": "edc69954-46e4-43d2-8d6e-f934e21a9086"
   },
   "outputs": [],
   "source": [
    "# X=np.array(np.load('/home/jupyter/curated_subset.npz')['X'])\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIv5qx9Li-iY",
    "outputId": "a841cecf-5db2-45a6-e6a9-881b5a3043f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>afrikaans</th>\n",
       "      <th>agni</th>\n",
       "      <th>akan</th>\n",
       "      <th>albanian</th>\n",
       "      <th>amazigh</th>\n",
       "      <th>amharic</th>\n",
       "      <th>arabic</th>\n",
       "      <th>armenian</th>\n",
       "      <th>ashanti</th>\n",
       "      <th>azerbaijani</th>\n",
       "      <th>...</th>\n",
       "      <th>wolof</th>\n",
       "      <th>wu</th>\n",
       "      <th>xasonga</th>\n",
       "      <th>xiang</th>\n",
       "      <th>yakut</th>\n",
       "      <th>yapese</th>\n",
       "      <th>yiddish</th>\n",
       "      <th>yoruba</th>\n",
       "      <th>yupik</th>\n",
       "      <th>zulu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2133 rows  214 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      afrikaans  agni  akan  albanian  amazigh  amharic  arabic  armenian  \\\n",
       "32          1.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "33          1.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "34          1.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "35          1.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "36          1.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "...         ...   ...   ...       ...      ...      ...     ...       ...   \n",
       "2167        0.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "2168        0.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "2169        0.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "2170        0.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "2171        0.0   0.0   0.0       0.0      0.0      0.0     0.0       0.0   \n",
       "\n",
       "      ashanti  azerbaijani  ...  wolof   wu  xasonga  xiang  yakut  yapese  \\\n",
       "32        0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "33        0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "34        0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "35        0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "36        0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "...       ...          ...  ...    ...  ...      ...    ...    ...     ...   \n",
       "2167      0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "2168      0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "2169      0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "2170      0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "2171      0.0          0.0  ...    0.0  0.0      0.0    0.0    0.0     0.0   \n",
       "\n",
       "      yiddish  yoruba  yupik  zulu  \n",
       "32        0.0     0.0    0.0   0.0  \n",
       "33        0.0     0.0    0.0   0.0  \n",
       "34        0.0     0.0    0.0   0.0  \n",
       "35        0.0     0.0    0.0   0.0  \n",
       "36        0.0     0.0    0.0   0.0  \n",
       "...       ...     ...    ...   ...  \n",
       "2167      0.0     1.0    0.0   0.0  \n",
       "2168      0.0     1.0    0.0   0.0  \n",
       "2169      0.0     1.0    0.0   0.0  \n",
       "2170      0.0     0.0    1.0   0.0  \n",
       "2171      0.0     0.0    0.0   1.0  \n",
       "\n",
       "[2133 rows x 214 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = df.iloc[:,14:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdDN4kIUi-ic",
    "outputId": "dfd4e3c6-b2cc-44f8-ac3a-8df2e6a51df6"
   },
   "outputs": [],
   "source": [
    "#df['number_labels'].mean()\n",
    "# Y=np.array(df.drop(['fname', 'labels','num_labels'], axis=1))\n",
    "# Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMPHUERZFXzg"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask zero out padding tokens.\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  return tf.matmul(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-F4_iB_KFXzj"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # linear layers\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # split heads\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xsfTXzebFXzn"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "    # apply sin to even index in the array\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd index in the array\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4pq5OKITFXzz"
   },
   "outputs": [],
   "source": [
    "# This allows to the transformer to know where there is real data and where it is padded\n",
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U389WZgtFXzq"
   },
   "outputs": [],
   "source": [
    "def encoder_layer(units, d_model, num_heads, dropout,name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,d_model ), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPEbBpxqFXzt"
   },
   "outputs": [],
   "source": [
    "def encoder(time_steps,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            projection,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "  \n",
    "  if projection=='linear':\n",
    "    ## We implement a linear projection based on Very Deep Self-Attention Networks for End-to-End Speech Recognition. Retrieved from https://arxiv.org/abs/1904.13377\n",
    "    projection=tf.keras.layers.Dense( d_model,use_bias=True, activation='linear')(inputs)\n",
    "    print('linear')\n",
    "  \n",
    "  else:\n",
    "    projection=tf.identity(inputs)\n",
    "    print('none')\n",
    "   \n",
    "  projection *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "  projection = PositionalEncoding(time_steps, d_model)(projection)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(projection)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    " \n",
    " \n",
    "  \n",
    "\n",
    " \n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n9uLLTcPFXzw"
   },
   "outputs": [],
   "source": [
    "def transformer(time_steps,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                output_size,\n",
    "                projection,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,d_model), name=\"inputs\")\n",
    "  \n",
    "  \n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(tf.dtypes.cast(\n",
    "          \n",
    "    #Like our input has a dimension of length X d_model but the masking is applied to a vector\n",
    "    # We get the sum for each row and result is a vector. So, if result is 0 it is because in that position was masked      \n",
    "    tf.math.reduce_sum(\n",
    "    inputs,\n",
    "    axis=2,\n",
    "    keepdims=False,\n",
    "    name=None\n",
    "), tf.int32))\n",
    "  \n",
    "\n",
    "  enc_outputs = encoder(\n",
    "      time_steps=time_steps,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "      projection=projection,\n",
    "      name='encoder'\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  #We reshape for feeding our FC in the next step\n",
    "  outputs=tf.reshape(enc_outputs,(-1,time_steps*d_model))\n",
    "  \n",
    "  #We predict our class\n",
    "  outputs = tf.keras.layers.Dense(units=output_size,use_bias=True,activation='softmax', name=\"outputs\")(outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs], outputs=outputs, name='audio_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OU4hWuxYFX0C"
   },
   "outputs": [],
   "source": [
    "#We get our train and test set\n",
    "X_train,X_test, Y_train, Y_test =train_test_split(X,Y, test_size=0.2, random_state=27)\n",
    "X_train,X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=0.2,random_state=27)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1364, 214)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1364, 646, 128) (342, 646, 128) (427, 646, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_val.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oT948RgHgKMJ"
   },
   "outputs": [],
   "source": [
    "projection=['linear','none']\n",
    "accuracy=[]\n",
    "proj_implemented=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2ifMZKEFX0J",
    "outputId": "6b38d307-bc5c-49ed-ad77-8df70d887c9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 391s 287ms/sample - loss: 5.9495 - accuracy: 0.0073 - val_loss: 5.3855 - val_accuracy: 0.0211\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 376s 276ms/sample - loss: 5.2094 - accuracy: 0.0359 - val_loss: 4.7870 - val_accuracy: 0.0843\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 362s 265ms/sample - loss: 4.6736 - accuracy: 0.0997 - val_loss: 4.4771 - val_accuracy: 0.1663\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 462s 339ms/sample - loss: 4.3706 - accuracy: 0.1437 - val_loss: 4.3619 - val_accuracy: 0.2248\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 583s 427ms/sample - loss: 4.2469 - accuracy: 0.1635 - val_loss: 4.3213 - val_accuracy: 0.2295\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 631s 463ms/sample - loss: 4.1770 - accuracy: 0.1686 - val_loss: 4.2985 - val_accuracy: 0.2389\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 515s 377ms/sample - loss: 4.0428 - accuracy: 0.1840 - val_loss: 4.2822 - val_accuracy: 0.2342\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 607s 445ms/sample - loss: 3.9975 - accuracy: 0.1862 - val_loss: 4.2758 - val_accuracy: 0.2365\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 556s 408ms/sample - loss: 3.9156 - accuracy: 0.1957 - val_loss: 4.2673 - val_accuracy: 0.2295\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 568s 417ms/sample - loss: 3.8519 - accuracy: 0.1884 - val_loss: 4.2682 - val_accuracy: 0.2365\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 538s 394ms/sample - loss: 5.9802 - accuracy: 0.0066 - val_loss: 5.3573 - val_accuracy: 0.0328\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 407s 299ms/sample - loss: 5.1622 - accuracy: 0.0440 - val_loss: 4.7343 - val_accuracy: 0.0867\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 414s 303ms/sample - loss: 4.6539 - accuracy: 0.0990 - val_loss: 4.4143 - val_accuracy: 0.1780\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 392s 287ms/sample - loss: 4.3684 - accuracy: 0.1488 - val_loss: 4.3093 - val_accuracy: 0.1944\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 426s 312ms/sample - loss: 4.2405 - accuracy: 0.1701 - val_loss: 4.2735 - val_accuracy: 0.2084\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 365s 268ms/sample - loss: 4.1093 - accuracy: 0.1811 - val_loss: 4.2555 - val_accuracy: 0.2131\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 380s 278ms/sample - loss: 4.0494 - accuracy: 0.1972 - val_loss: 4.2467 - val_accuracy: 0.2201\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 393s 288ms/sample - loss: 3.9775 - accuracy: 0.1870 - val_loss: 4.2390 - val_accuracy: 0.2155\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 405s 297ms/sample - loss: 3.8969 - accuracy: 0.1957 - val_loss: 4.2351 - val_accuracy: 0.2155\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 442s 324ms/sample - loss: 3.8082 - accuracy: 0.1913 - val_loss: 4.2348 - val_accuracy: 0.2225\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 380s 279ms/sample - loss: 6.2141 - accuracy: 0.0044 - val_loss: 5.5934 - val_accuracy: 0.0070\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 326s 239ms/sample - loss: 5.2055 - accuracy: 0.0293 - val_loss: 4.7833 - val_accuracy: 0.0656\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 322s 236ms/sample - loss: 4.5827 - accuracy: 0.0924 - val_loss: 4.3554 - val_accuracy: 0.1522\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 322s 236ms/sample - loss: 4.2770 - accuracy: 0.1672 - val_loss: 4.2406 - val_accuracy: 0.1991\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 321s 235ms/sample - loss: 4.1263 - accuracy: 0.1818 - val_loss: 4.2065 - val_accuracy: 0.2131\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 315s 231ms/sample - loss: 4.0770 - accuracy: 0.1833 - val_loss: 4.1808 - val_accuracy: 0.2178\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 311s 228ms/sample - loss: 4.0072 - accuracy: 0.1987 - val_loss: 4.1677 - val_accuracy: 0.2319\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 313s 229ms/sample - loss: 3.9342 - accuracy: 0.1943 - val_loss: 4.1612 - val_accuracy: 0.2365\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 314s 230ms/sample - loss: 3.8666 - accuracy: 0.2009 - val_loss: 4.1531 - val_accuracy: 0.2272\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 312s 228ms/sample - loss: 3.8112 - accuracy: 0.2067 - val_loss: 4.1487 - val_accuracy: 0.2412\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 323s 237ms/sample - loss: 6.3318 - accuracy: 7.3314e-04 - val_loss: 5.9376 - val_accuracy: 0.0070\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 314s 230ms/sample - loss: 5.6075 - accuracy: 0.0169 - val_loss: 5.3128 - val_accuracy: 0.0211\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 311s 228ms/sample - loss: 5.0637 - accuracy: 0.0308 - val_loss: 4.8368 - val_accuracy: 0.0632\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.5888 - accuracy: 0.0799 - val_loss: 4.5250 - val_accuracy: 0.1265\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.3327 - accuracy: 0.1210 - val_loss: 4.3633 - val_accuracy: 0.1756\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.2130 - accuracy: 0.1584 - val_loss: 4.2942 - val_accuracy: 0.2014\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 311s 228ms/sample - loss: 4.0782 - accuracy: 0.1664 - val_loss: 4.2616 - val_accuracy: 0.2201\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 3.9810 - accuracy: 0.1767 - val_loss: 4.2414 - val_accuracy: 0.2248\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 312s 228ms/sample - loss: 3.9406 - accuracy: 0.1994 - val_loss: 4.2318 - val_accuracy: 0.2248\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 315s 231ms/sample - loss: 3.8758 - accuracy: 0.1921 - val_loss: 4.2265 - val_accuracy: 0.2248\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 322s 236ms/sample - loss: 5.7237 - accuracy: 0.0147 - val_loss: 4.9409 - val_accuracy: 0.0632\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.9370 - accuracy: 0.0667 - val_loss: 4.3913 - val_accuracy: 0.1241\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 314s 230ms/sample - loss: 4.4555 - accuracy: 0.1540 - val_loss: 4.2013 - val_accuracy: 0.2108\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.2811 - accuracy: 0.1650 - val_loss: 4.1495 - val_accuracy: 0.2248\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 311s 228ms/sample - loss: 4.1508 - accuracy: 0.1833 - val_loss: 4.1244 - val_accuracy: 0.2295\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.1164 - accuracy: 0.1848 - val_loss: 4.1077 - val_accuracy: 0.2295\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 313s 229ms/sample - loss: 4.0286 - accuracy: 0.1935 - val_loss: 4.0952 - val_accuracy: 0.2295\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 311s 228ms/sample - loss: 3.9865 - accuracy: 0.1811 - val_loss: 4.0906 - val_accuracy: 0.2342\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 3.8903 - accuracy: 0.1848 - val_loss: 4.0876 - val_accuracy: 0.2319\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 312s 228ms/sample - loss: 3.8457 - accuracy: 0.1950 - val_loss: 4.0832 - val_accuracy: 0.2342\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 324s 237ms/sample - loss: 5.6423 - accuracy: 0.0315 - val_loss: 4.9235 - val_accuracy: 0.1218\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 4.8424 - accuracy: 0.1254 - val_loss: 4.4244 - val_accuracy: 0.2365\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1364/1364 [==============================] - 312s 228ms/sample - loss: 4.4701 - accuracy: 0.1789 - val_loss: 4.2587 - val_accuracy: 0.2482\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 313s 230ms/sample - loss: 4.2535 - accuracy: 0.1906 - val_loss: 4.1988 - val_accuracy: 0.2389\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 313s 229ms/sample - loss: 4.2157 - accuracy: 0.1752 - val_loss: 4.1774 - val_accuracy: 0.2319\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 311s 228ms/sample - loss: 4.0837 - accuracy: 0.1701 - val_loss: 4.1686 - val_accuracy: 0.2365\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 312s 229ms/sample - loss: 3.9977 - accuracy: 0.1870 - val_loss: 4.1629 - val_accuracy: 0.2436\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 348s 255ms/sample - loss: 3.9935 - accuracy: 0.1877 - val_loss: 4.1567 - val_accuracy: 0.2412\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 342s 250ms/sample - loss: 3.8986 - accuracy: 0.1884 - val_loss: 4.1539 - val_accuracy: 0.2412\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 371s 272ms/sample - loss: 3.8238 - accuracy: 0.1965 - val_loss: 4.1522 - val_accuracy: 0.2412\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 369s 271ms/sample - loss: 5.8449 - accuracy: 0.0095 - val_loss: 5.3085 - val_accuracy: 0.0281\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 335s 245ms/sample - loss: 5.0150 - accuracy: 0.0535 - val_loss: 4.7168 - val_accuracy: 0.1101\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 313s 230ms/sample - loss: 4.5057 - accuracy: 0.1261 - val_loss: 4.4728 - val_accuracy: 0.1920\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 314s 230ms/sample - loss: 4.3417 - accuracy: 0.1496 - val_loss: 4.4001 - val_accuracy: 0.2037\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 325s 238ms/sample - loss: 4.1998 - accuracy: 0.1723 - val_loss: 4.3646 - val_accuracy: 0.1991\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 378s 277ms/sample - loss: 4.0745 - accuracy: 0.1774 - val_loss: 4.3409 - val_accuracy: 0.1967\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 394s 289ms/sample - loss: 4.0351 - accuracy: 0.1840 - val_loss: 4.3301 - val_accuracy: 0.2037\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 362s 265ms/sample - loss: 3.9513 - accuracy: 0.1884 - val_loss: 4.3150 - val_accuracy: 0.2037\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 382s 280ms/sample - loss: 3.8575 - accuracy: 0.1950 - val_loss: 4.3097 - val_accuracy: 0.2014\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 396s 290ms/sample - loss: 3.8228 - accuracy: 0.2031 - val_loss: 4.3055 - val_accuracy: 0.2061\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 471s 345ms/sample - loss: 5.6842 - accuracy: 0.0110 - val_loss: 5.0435 - val_accuracy: 0.0468\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 446s 327ms/sample - loss: 4.8691 - accuracy: 0.0652 - val_loss: 4.4888 - val_accuracy: 0.1429\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 377s 277ms/sample - loss: 4.4114 - accuracy: 0.1327 - val_loss: 4.2859 - val_accuracy: 0.2225\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 419s 307ms/sample - loss: 4.2574 - accuracy: 0.1540 - val_loss: 4.2349 - val_accuracy: 0.2436\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 413s 303ms/sample - loss: 4.1324 - accuracy: 0.1840 - val_loss: 4.2069 - val_accuracy: 0.2482\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 383s 281ms/sample - loss: 4.0301 - accuracy: 0.1848 - val_loss: 4.1960 - val_accuracy: 0.2506\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 370s 271ms/sample - loss: 3.9485 - accuracy: 0.1935 - val_loss: 4.1866 - val_accuracy: 0.2482\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 381s 280ms/sample - loss: 3.9658 - accuracy: 0.1891 - val_loss: 4.1779 - val_accuracy: 0.2506\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 362s 265ms/sample - loss: 3.8839 - accuracy: 0.1950 - val_loss: 4.1788 - val_accuracy: 0.2529\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 348s 255ms/sample - loss: 3.8384 - accuracy: 0.1870 - val_loss: 4.1756 - val_accuracy: 0.2600\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 380s 279ms/sample - loss: 5.9097 - accuracy: 0.0227 - val_loss: 5.3651 - val_accuracy: 0.0422\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 326s 239ms/sample - loss: 5.1877 - accuracy: 0.0587 - val_loss: 4.7875 - val_accuracy: 0.1499\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 339s 249ms/sample - loss: 4.6336 - accuracy: 0.1180 - val_loss: 4.4987 - val_accuracy: 0.1944\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 355s 260ms/sample - loss: 4.4052 - accuracy: 0.1503 - val_loss: 4.3747 - val_accuracy: 0.2084\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 387s 284ms/sample - loss: 4.2369 - accuracy: 0.1664 - val_loss: 4.3145 - val_accuracy: 0.2155\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 354s 260ms/sample - loss: 4.0747 - accuracy: 0.1811 - val_loss: 4.2810 - val_accuracy: 0.2295\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 344s 252ms/sample - loss: 4.0694 - accuracy: 0.1701 - val_loss: 4.2543 - val_accuracy: 0.2389\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 412s 302ms/sample - loss: 3.9894 - accuracy: 0.1840 - val_loss: 4.2385 - val_accuracy: 0.2342\n",
      "Epoch 9/10\n",
      "1364/1364 [==============================] - 415s 304ms/sample - loss: 3.9355 - accuracy: 0.1752 - val_loss: 4.2286 - val_accuracy: 0.2365\n",
      "Epoch 10/10\n",
      "1364/1364 [==============================] - 428s 314ms/sample - loss: 3.8088 - accuracy: 0.2009 - val_loss: 4.2223 - val_accuracy: 0.2389\n",
      "linear\n",
      "Train on 1364 samples, validate on 427 samples\n",
      "Epoch 1/10\n",
      "1364/1364 [==============================] - 399s 292ms/sample - loss: 6.2184 - accuracy: 0.0088 - val_loss: 5.8379 - val_accuracy: 0.0187\n",
      "Epoch 2/10\n",
      "1364/1364 [==============================] - 389s 285ms/sample - loss: 5.3867 - accuracy: 0.0220 - val_loss: 5.1372 - val_accuracy: 0.0562\n",
      "Epoch 3/10\n",
      "1364/1364 [==============================] - 372s 272ms/sample - loss: 4.7758 - accuracy: 0.0565 - val_loss: 4.6624 - val_accuracy: 0.1030\n",
      "Epoch 4/10\n",
      "1364/1364 [==============================] - 328s 241ms/sample - loss: 4.4092 - accuracy: 0.1041 - val_loss: 4.4029 - val_accuracy: 0.1686\n",
      "Epoch 5/10\n",
      "1364/1364 [==============================] - 372s 273ms/sample - loss: 4.2257 - accuracy: 0.1422 - val_loss: 4.3075 - val_accuracy: 0.1967\n",
      "Epoch 6/10\n",
      "1364/1364 [==============================] - 396s 290ms/sample - loss: 4.1226 - accuracy: 0.1686 - val_loss: 4.2737 - val_accuracy: 0.2225\n",
      "Epoch 7/10\n",
      "1364/1364 [==============================] - 405s 297ms/sample - loss: 4.0249 - accuracy: 0.1833 - val_loss: 4.2631 - val_accuracy: 0.2319\n",
      "Epoch 8/10\n",
      "1364/1364 [==============================] - 383s 281ms/sample - loss: 3.9742 - accuracy: 0.1774 - val_loss: 4.2540 - val_accuracy: 0.2436\n",
      "Epoch 9/10\n",
      "  64/1364 [>.............................] - ETA: 4:53 - loss: 3.9750 - accuracy: 0.1875"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-442967568b84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/dev/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in projection:\n",
    "  NUM_LAYERS = 2\n",
    "  D_MODEL = X.shape[2]\n",
    "  NUM_HEADS = 4\n",
    "  UNITS = 1024\n",
    "  DROPOUT = 0.1\n",
    "  TIME_STEPS= X.shape[1]\n",
    "  OUTPUT_SIZE=214\n",
    "  EPOCHS = 10\n",
    "  EXPERIMENTS=10\n",
    "\n",
    "  \n",
    "  for j in range(EXPERIMENTS):\n",
    "    \n",
    "    \n",
    "    model = transformer(time_steps=TIME_STEPS,\n",
    "      num_layers=NUM_LAYERS,\n",
    "      units=UNITS,\n",
    "      d_model=D_MODEL,\n",
    "      num_heads=NUM_HEADS,\n",
    "      dropout=DROPOUT,\n",
    "      output_size=OUTPUT_SIZE,  \n",
    "      projection=i  )\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(0.000001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    history=model.fit(X_train,Y_train, epochs=EPOCHS, validation_data=(X_test, Y_test))\n",
    "\n",
    "    accuracy.append(max(history.history['val_accuracy']))\n",
    "      \n",
    "    proj_implemented.append(i)\n",
    "    \n",
    "    del model\n",
    "    \n",
    "    del history\n",
    "      \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chxvhHsAFX02"
   },
   "outputs": [],
   "source": [
    "accuracy=pd.DataFrame(accuracy, columns=['accuracy'])\n",
    "proj_implemented=pd.DataFrame(proj_implemented, columns=['projection'])\n",
    "results=pd.concat([accuracy,proj_implemented],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LJeC1iwccFA",
    "outputId": "021aea92-09b4-43c5-b9b0-6494948a8026"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projection</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.102213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>0.086821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy\n",
       "projection          \n",
       "linear      0.102213\n",
       "none        0.086821"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby('projection').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bA0Y-5O0dUtz",
    "outputId": "9ffd8d73-97aa-4eb1-e885-9067a6046086"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>projection</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>linear</th>\n",
       "      <td>0.017835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>0.018532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            accuracy\n",
       "projection          \n",
       "linear      0.017835\n",
       "none        0.018532"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.groupby('projection').std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1cYVHkci-jJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.768299319727891"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"duration\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BXYtzMni-jM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "audio_classification_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
