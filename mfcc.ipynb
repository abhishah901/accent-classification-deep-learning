{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\Downloads\\Jupyter\\accent-classification-deep-learning-master\\speech-accent-archive\\recordings\\recordings\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\karth\\\\Downloads\\\\Jupyter\\\\accent-classification-deep-learning-master\\\\speech-accent-archive\\\\recordings\\\\recordings\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['filename','mfcc','accent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_list(note):\n",
    "    y, trim_idx = librosa.effects.trim(note)  # \n",
    "    length= 5  # voice note length(seconds)\n",
    "    sr= 10000     #sampling rate\n",
    "    len_sample= length* sr #array length of sample\n",
    "    ol= 1  #overlap length(seconds)\n",
    "    len_ol= ol* sr #overlaplength(array)\n",
    "    y_mat=[] #initiate list\n",
    "    i=1 #iterator\n",
    "\n",
    "    if(trim_idx[1]<len_sample):  #check if voice note is too small\n",
    "        return y_mat             #return null\n",
    "    else:\n",
    "        while(i*len_sample-(i-1)*len_ol<= trim_idx[1]):                             \n",
    "            trim_y=y[(i-1)* len_sample-(i-1)*len_ol: i*len_sample-(i-1)*len_ol]      #trim voice notes\n",
    "            y_mat.append(trim_y)\n",
    "            i=i+1\n",
    "        if((i-1)*len_sample-(i-2)*len_ol<trim_idx[1]):\n",
    "            trim_y=y[trim_idx[1]-len_sample:trim_idx[1] ]                 #additionof remainder voice note\n",
    "            y_mat.append(trim_y)\n",
    "    \n",
    "        return y_mat                                                  #return list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_train=[]\n",
    "row_test=[]\n",
    "\n",
    "\n",
    "counter=4\n",
    "c=0\n",
    "\n",
    "for each in os.listdir():\n",
    "    c+=1\n",
    "    accent = re.split('\\d',each)[0]\n",
    "    waveform, sr = librosa.load(each, sr=SAMPLE_RATE)\n",
    "    y_mat = get_audio_list(waveform)\n",
    "    \n",
    "#     print(\"ymat\",each,y_mat )\n",
    "    if(c%counter==0):\n",
    "        mfcc_test=[]\n",
    "        for i in range(len(y_mat)):\n",
    "            mfcc = librosa.feature.mfcc(y_mat[i], sr=SAMPLE_RATE,n_mfcc=20)\n",
    "#             print(\"ymat\",y_mat[i].shape)\n",
    "            mfcc = torch.from_numpy(mfcc)\n",
    "#             print(\"mfcc\",mfcc.shape,each)\n",
    "            if(mfcc.shape[1]==98):\n",
    "#                 print(mfcc.shape,each)\n",
    "                mfcc_test.append(mfcc)\n",
    "            \n",
    "        row_test.append({'filename': each,'mfcc': mfcc_test,'accent': accent})\n",
    "#             print(\"test\",c)\n",
    "    else:\n",
    "        mfcc_train=[]\n",
    "        for i in range(len(y_mat)):\n",
    "            mfcc = librosa.feature.mfcc(y_mat[i], sr=SAMPLE_RATE,n_mfcc=20)\n",
    "            mfcc = torch.from_numpy(mfcc)\n",
    "#             print(\"mfcc\",mfcc.shape,each)\n",
    "            if(mfcc.shape[1]==98):\n",
    "#                 print(mfcc.shape,each)\n",
    "                mfcc_train.append(mfcc)\n",
    "#         print(\"train\",len(y_mat))\n",
    "#         print(\"mfcc\",len(mfcc_train))\n",
    "        row_train.append({'filename': each,'mfcc': mfcc_train,'accent': accent})\n",
    "#             \n",
    "#         print(\"mfcc\",i,mfcc_final[i])\n",
    "#         print(\"\",mfcc_final)\n",
    "#     if c==10:break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604 534\n"
     ]
    }
   ],
   "source": [
    "print(len(row_train),len(row_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(row_train)\n",
    "df_test = pd.DataFrame(row_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = df_train[[\"accent\"]].values\n",
    "unique_values =  np.unique(column_values)\n",
    "unique_values = unique_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_val = df_test[[\"accent\"]].values\n",
    "unique_val =  np.unique(column_val)\n",
    "unique_val = unique_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['accent']=df_train['accent'].apply(unique_values.index)\n",
    "df_test['accent']=df_test['accent'].apply(unique_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of             filename                                               mfcc  \\\n",
       "0     afrikaans1.mp3  [[[tensor(-356.1865, dtype=torch.float64), ten...   \n",
       "1     afrikaans2.mp3  [[[tensor(-559.4249, dtype=torch.float64), ten...   \n",
       "2     afrikaans3.mp3  [[[tensor(-410.6700, dtype=torch.float64), ten...   \n",
       "3     afrikaans5.mp3  [[[tensor(-445.6351, dtype=torch.float64), ten...   \n",
       "4          agni1.mp3  [[[tensor(-235.7196, dtype=torch.float64), ten...   \n",
       "...              ...                                                ...   \n",
       "1599     yoruba2.mp3  [[[tensor(-347.5009, dtype=torch.float64), ten...   \n",
       "1600     yoruba3.mp3  [[[tensor(-301.6995, dtype=torch.float64), ten...   \n",
       "1601     yoruba4.mp3  [[[tensor(-351.1957, dtype=torch.float64), ten...   \n",
       "1602      yupik1.mp3  [[[tensor(-353.3956, dtype=torch.float64), ten...   \n",
       "1603       zulu1.mp3  [[[tensor(-603.7276, dtype=torch.float64), ten...   \n",
       "\n",
       "      accent  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "...      ...  \n",
       "1599     181  \n",
       "1600     181  \n",
       "1601     181  \n",
       "1602     182  \n",
       "1603     183  \n",
       "\n",
       "[1604 rows x 3 columns]>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, df_train):  \n",
    "        self.dataset = df_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'mfcc': df_train.iloc[idx].values[1], 'labels': df_train.iloc[idx].values[2]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=AudioDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test=AudioDataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrikaans1.mp3</td>\n",
       "      <td>[[[tensor(-356.1865, dtype=torch.float64), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrikaans2.mp3</td>\n",
       "      <td>[[[tensor(-559.4249, dtype=torch.float64), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrikaans3.mp3</td>\n",
       "      <td>[[[tensor(-410.6700, dtype=torch.float64), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrikaans5.mp3</td>\n",
       "      <td>[[[tensor(-445.6351, dtype=torch.float64), ten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agni1.mp3</td>\n",
       "      <td>[[[tensor(-235.7196, dtype=torch.float64), ten...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>yoruba2.mp3</td>\n",
       "      <td>[[[tensor(-347.5009, dtype=torch.float64), ten...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>yoruba3.mp3</td>\n",
       "      <td>[[[tensor(-301.6995, dtype=torch.float64), ten...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>yoruba4.mp3</td>\n",
       "      <td>[[[tensor(-351.1957, dtype=torch.float64), ten...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>yupik1.mp3</td>\n",
       "      <td>[[[tensor(-353.3956, dtype=torch.float64), ten...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>zulu1.mp3</td>\n",
       "      <td>[[[tensor(-603.7276, dtype=torch.float64), ten...</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1604 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                                               mfcc  \\\n",
       "0     afrikaans1.mp3  [[[tensor(-356.1865, dtype=torch.float64), ten...   \n",
       "1     afrikaans2.mp3  [[[tensor(-559.4249, dtype=torch.float64), ten...   \n",
       "2     afrikaans3.mp3  [[[tensor(-410.6700, dtype=torch.float64), ten...   \n",
       "3     afrikaans5.mp3  [[[tensor(-445.6351, dtype=torch.float64), ten...   \n",
       "4          agni1.mp3  [[[tensor(-235.7196, dtype=torch.float64), ten...   \n",
       "...              ...                                                ...   \n",
       "1599     yoruba2.mp3  [[[tensor(-347.5009, dtype=torch.float64), ten...   \n",
       "1600     yoruba3.mp3  [[[tensor(-301.6995, dtype=torch.float64), ten...   \n",
       "1601     yoruba4.mp3  [[[tensor(-351.1957, dtype=torch.float64), ten...   \n",
       "1602      yupik1.mp3  [[[tensor(-353.3956, dtype=torch.float64), ten...   \n",
       "1603       zulu1.mp3  [[[tensor(-603.7276, dtype=torch.float64), ten...   \n",
       "\n",
       "      accent  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "...      ...  \n",
       "1599     181  \n",
       "1600     181  \n",
       "1601     181  \n",
       "1602     182  \n",
       "1603     183  \n",
       "\n",
       "[1604 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_load = torch.utils.data.DataLoader(dataset=df_train,\n",
    "#                                            batch_size=batch_size,\n",
    "#                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_load = torch.utils.data.DataLoader(dataset=dataset_test,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)        #Batch normalization\n",
    "        self.relu = nn.ReLU()                 #RELU Activation\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)   #Maxpooling reduces the size by kernel size. 64/2 = 32\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)    #Size now is 32/2 = 16\n",
    "        \n",
    "        #Flatten the feature maps. You have 32 feature mapsfrom cnn2. Each of the feature is of size 16x16 --> 32*16*16 = 8192\n",
    "        self.fc1 = nn.Linear(in_features=3840, out_features=4000)   #Flattened image is fed into linear NN and reduced to half size\n",
    "        self.droput = nn.Dropout(p=0.5)                    #Dropout used to reduce overfitting\n",
    "        self.fc2 = nn.Linear(in_features=4000, out_features=2000)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(in_features=2000, out_features=500)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc4 = nn.Linear(in_features=500, out_features=50)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc5 = nn.Linear(in_features=50, out_features=220)    #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n",
    "       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        #Flattening is done here with .view() -> (batch_size, 32*16*16) = (100, 8192)\n",
    "        out = out.view(-1,3840)   #-1 will automatically update the batchsize as 100; 8192 flattens 32,16,16\n",
    "        #Then we forward through our fully connected layer \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    model = model.cuda()    \n",
    "loss_fn = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150, Training Loss: 3.293, Training Accuracy: 29.000\n",
      "Epoch 2/150, Training Loss: 3.276, Training Accuracy: 30.000\n",
      "Epoch 3/150, Training Loss: 3.234, Training Accuracy: 30.000\n",
      "Epoch 4/150, Training Loss: 3.180, Training Accuracy: 31.000\n",
      "Epoch 5/150, Training Loss: 3.152, Training Accuracy: 31.000\n",
      "Epoch 6/150, Training Loss: 3.108, Training Accuracy: 32.000\n",
      "Epoch 7/150, Training Loss: 3.085, Training Accuracy: 33.000\n",
      "Epoch 8/150, Training Loss: 3.037, Training Accuracy: 33.000\n",
      "Epoch 9/150, Training Loss: 2.990, Training Accuracy: 34.000\n",
      "Epoch 10/150, Training Loss: 2.978, Training Accuracy: 34.000\n",
      "Epoch 11/150, Training Loss: 2.930, Training Accuracy: 35.000\n",
      "Epoch 12/150, Training Loss: 2.878, Training Accuracy: 35.000\n",
      "Epoch 13/150, Training Loss: 2.843, Training Accuracy: 36.000\n",
      "Epoch 14/150, Training Loss: 2.838, Training Accuracy: 37.000\n",
      "Epoch 15/150, Training Loss: 2.825, Training Accuracy: 37.000\n",
      "Epoch 16/150, Training Loss: 2.737, Training Accuracy: 38.000\n",
      "Epoch 17/150, Training Loss: 2.716, Training Accuracy: 38.000\n",
      "Epoch 18/150, Training Loss: 2.641, Training Accuracy: 39.000\n",
      "Epoch 19/150, Training Loss: 2.638, Training Accuracy: 40.000\n",
      "Epoch 20/150, Training Loss: 2.650, Training Accuracy: 40.000\n",
      "Epoch 21/150, Training Loss: 2.624, Training Accuracy: 40.000\n",
      "Epoch 22/150, Training Loss: 2.599, Training Accuracy: 41.000\n",
      "Epoch 23/150, Training Loss: 2.558, Training Accuracy: 41.000\n",
      "Epoch 24/150, Training Loss: 2.528, Training Accuracy: 42.000\n",
      "Epoch 25/150, Training Loss: 2.527, Training Accuracy: 41.000\n",
      "Epoch 26/150, Training Loss: 2.480, Training Accuracy: 42.000\n",
      "Epoch 27/150, Training Loss: 2.504, Training Accuracy: 42.000\n",
      "Epoch 28/150, Training Loss: 2.484, Training Accuracy: 42.000\n",
      "Epoch 29/150, Training Loss: 2.479, Training Accuracy: 43.000\n",
      "Epoch 30/150, Training Loss: 2.477, Training Accuracy: 43.000\n",
      "Epoch 31/150, Training Loss: 2.528, Training Accuracy: 42.000\n",
      "Epoch 32/150, Training Loss: 2.466, Training Accuracy: 43.000\n",
      "Epoch 33/150, Training Loss: 2.489, Training Accuracy: 43.000\n",
      "Epoch 34/150, Training Loss: 2.476, Training Accuracy: 43.000\n",
      "Epoch 35/150, Training Loss: 2.453, Training Accuracy: 44.000\n",
      "Epoch 36/150, Training Loss: 2.525, Training Accuracy: 43.000\n",
      "Epoch 37/150, Training Loss: 2.536, Training Accuracy: 43.000\n",
      "Epoch 38/150, Training Loss: 2.542, Training Accuracy: 43.000\n",
      "Epoch 39/150, Training Loss: 2.562, Training Accuracy: 43.000\n",
      "Epoch 40/150, Training Loss: 2.516, Training Accuracy: 43.000\n"
     ]
    }
   ],
   "source": [
    "#Training the CNN\n",
    "\n",
    "import time\n",
    "\n",
    "num_epochs = 150\n",
    "\n",
    "#Define the lists to store the results of loss and accuracy\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(num_epochs): \n",
    "    #Reset these below variables to 0 at the begining of every epoch\n",
    "    start = time.time()\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, data in enumerate(train_load):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "#         print(\"ymat\",i,len(data['mfcc']))\n",
    "#         for i in data['mfcc']:\n",
    "#             print(\"mfcc\", i.shape)\n",
    "        \n",
    "        for j in range(len(data['mfcc'])-1):\n",
    "            inputs = data['mfcc'][j]\n",
    "            inputs = torch.FloatTensor(np.expand_dims(inputs,axis=1))\n",
    "#             print(\"inputs\",inputs.shape)\n",
    "#             print(\"labels length\",len(data['labels']),data['labels'])\n",
    "#             print(j,data['labels'])\n",
    "            labels = Variable(data['labels'].view(1))\n",
    "            \n",
    "            \n",
    "            #print(\"labels\",labels)\n",
    "            # If we have GPU, shift the data to GPU\n",
    "            CUDA = torch.cuda.is_available()\n",
    "            if CUDA:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()            # Clear off the gradient in (w = w - gradient)\n",
    "            outputs = model(inputs) \n",
    "            #print(\"outputs\",outputs.shape,\"label\",labels.shape)\n",
    "            loss = loss_fn(outputs, labels)  \n",
    "            iter_loss += loss.item()       # Accumulate the loss\n",
    "            loss.backward()                 # Backpropagation \n",
    "            optimizer.step()                # Update the weights\n",
    "\n",
    "            # Record the correct predictions for training data \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append(100 * correct / iterations)\n",
    "\n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}'\n",
    "            .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1]))\n",
    "\n",
    "    #Testing\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "#     model.eval()                    # Put the network into evaluation mode\n",
    "    \n",
    "#     for i, (inputs, labels) in enumerate(test_load):\n",
    "        \n",
    "#         # Convert torch tensor to Variable\n",
    "#         inputs = Variable(inputs)\n",
    "#         labels = Variable(labels)\n",
    "        \n",
    "#         CUDA = torch.cuda.is_available()\n",
    "#         if CUDA:\n",
    "#             inputs = inputs.cuda()\n",
    "#             labels = labels.cuda()\n",
    "        \n",
    "#         outputs = model(inputs)     \n",
    "#         loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "#         loss += loss.data[0]\n",
    "#         # Record the correct predictions for training data\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct += (predicted == labels).sum()\n",
    "        \n",
    "#         iterations += 1\n",
    "\n",
    "#     # Record the Testing loss\n",
    "#     test_loss.append(loss/iterations)\n",
    "#     # Record the Testing accuracy\n",
    "#     test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "#     stop = time.time()\n",
    "    \n",
    "#     print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}, Time: {}s'\n",
    "#            .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1], stop-start))\n",
    "\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()                    # Put the network into evaluation mode\n",
    "    \n",
    "    for i, data in enumerate(test_load):\n",
    "        \n",
    "\n",
    "        for j in range(len(data['mfcc'])-1):\n",
    "            inputs = data['mfcc'][j]\n",
    "            inputs = torch.FloatTensor(np.expand_dims(inputs,axis=1))\n",
    "            labels = Variable(data['labels'].view(1))\n",
    "    \n",
    "    \n",
    "        CUDA = torch.cuda.is_available()\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        outputs = model(inputs)     \n",
    "        loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "        loss += loss.data[0]\n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the Testing loss\n",
    "    test_loss.append(loss/iterations)\n",
    "    # Record the Testing accuracy\n",
    "    test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "    stop = time.time()\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}, Time: {}s'\n",
    "           .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1], stop-start))\n",
    "\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
