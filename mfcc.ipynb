{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karth\\Downloads\\Jupyter\\accent-classification-deep-learning-master\\speech-accent-archive\\recordings\\recordings\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\karth\\\\Downloads\\\\Jupyter\\\\accent-classification-deep-learning-master\\\\speech-accent-archive\\\\recordings\\\\recordings\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['filename','mfcc','accent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_list(note):\n",
    "    y, trim_idx = librosa.effects.trim(note)  # \n",
    "    length= 5  # voice note length(seconds)\n",
    "    sr= 10000     #sampling rate\n",
    "    len_sample= length* sr #array length of sample\n",
    "    ol= 1  #overlap length(seconds)\n",
    "    len_ol= ol* sr #overlaplength(array)\n",
    "    y_mat=[] #initiate list\n",
    "    i=1 #iterator\n",
    "\n",
    "    if(trim_idx[1]<len_sample):  #check if voice note is too small\n",
    "        return y_mat             #return null\n",
    "    else:\n",
    "        while(i*len_sample-(i-1)*len_ol<= trim_idx[1]):                             \n",
    "            trim_y=y[(i-1)* len_sample-(i-1)*len_ol: i*len_sample-(i-1)*len_ol]      #trim voice notes\n",
    "            y_mat.append(trim_y)\n",
    "            i=i+1\n",
    "        if((i-1)*len_sample-(i-2)*len_ol<trim_idx[1]):\n",
    "            trim_y=y[trim_idx[1]-len_sample:trim_idx[1] ]                 #additionof remainder voice note\n",
    "            y_mat.append(trim_y)\n",
    "    \n",
    "        return y_mat                                                  #return list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_train=[]\n",
    "row_test=[]\n",
    "\n",
    "\n",
    "counter=4\n",
    "c=0\n",
    "\n",
    "for each in os.listdir():\n",
    "    c+=1\n",
    "    accent = re.split('\\d',each)[0]\n",
    "    waveform, sr = librosa.load(each, sr=SAMPLE_RATE)\n",
    "    y_mat = get_audio_list(waveform)\n",
    "    \n",
    "#     print(\"ymat\",each,y_mat )\n",
    "    if(c%counter==0):\n",
    "        mfcc_test=[]\n",
    "        for i in range(len(y_mat)):\n",
    "            mfcc = librosa.feature.mfcc(y_mat[i], sr=SAMPLE_RATE,n_mfcc=20)\n",
    "#             print(\"ymat\",y_mat[i].shape)\n",
    "            mfcc = torch.from_numpy(mfcc)\n",
    "#             print(\"mfcc\",mfcc.shape,each)\n",
    "            if(mfcc.shape[1]==98):\n",
    "#                 print(mfcc.shape,each)\n",
    "                mfcc_test.append(mfcc)\n",
    "            \n",
    "        row_test.append({'filename': each,'mfcc': mfcc_test,'accent': accent})\n",
    "#             print(\"test\",c)\n",
    "    else:\n",
    "        mfcc_train=[]\n",
    "        for i in range(len(y_mat)):\n",
    "            mfcc = librosa.feature.mfcc(y_mat[i], sr=SAMPLE_RATE,n_mfcc=20)\n",
    "            mfcc = torch.from_numpy(mfcc)\n",
    "#             print(\"mfcc\",mfcc.shape,each)\n",
    "            if(mfcc.shape[1]==98):\n",
    "#                 print(mfcc.shape,each)\n",
    "                mfcc_train.append(mfcc)\n",
    "#         print(\"train\",len(y_mat))\n",
    "#         print(\"mfcc\",len(mfcc_train))\n",
    "        row_train.append({'filename': each,'mfcc': mfcc_train,'accent': accent})\n",
    "#             \n",
    "#         print(\"mfcc\",i,mfcc_final[i])\n",
    "#         print(\"\",mfcc_final)\n",
    "#     if c==10:break\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(row_train),len(row_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(row_train)\n",
    "df_test = pd.DataFrame(row_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_values = df_train[[\"accent\"]].values\n",
    "unique_values =  np.unique(column_values)\n",
    "unique_values = unique_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_val = df_test[[\"accent\"]].values\n",
    "unique_val =  np.unique(column_val)\n",
    "unique_val = unique_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['accent']=df_train['accent'].apply(unique_values.index)\n",
    "df_test['accent']=df_test['accent'].apply(unique_val.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, df_train):\n",
    "        \n",
    "      \n",
    "        self.dataset = df_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {'mfcc': df_train.iloc[idx].values[1], 'labels': df_train.iloc[idx].values[2]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=AudioDataset(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_load = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_load = torch.utils.data.DataLoader(dataset=df_train,\n",
    "#                                            batch_size=batch_size,\n",
    "#                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_load = torch.utils.data.DataLoader(dataset=df_test,\n",
    "#                                            batch_size=batch_size,\n",
    "#                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        \n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)        #Batch normalization\n",
    "        self.relu = nn.ReLU()                 #RELU Activation\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)   #Maxpooling reduces the size by kernel size. 64/2 = 32\n",
    "        \n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)    #Size now is 32/2 = 16\n",
    "        \n",
    "        #Flatten the feature maps. You have 32 feature mapsfrom cnn2. Each of the feature is of size 16x16 --> 32*16*16 = 8192\n",
    "        self.fc1 = nn.Linear(in_features=55040, out_features=4000)   #Flattened image is fed into linear NN and reduced to half size\n",
    "        self.droput = nn.Dropout(p=0.5)                    #Dropout used to reduce overfitting\n",
    "        self.fc2 = nn.Linear(in_features=4000, out_features=2000)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc3 = nn.Linear(in_features=2000, out_features=500)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc4 = nn.Linear(in_features=500, out_features=50)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc5 = nn.Linear(in_features=50, out_features=220)    #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n",
    "       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.batchnorm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        #Flattening is done here with .view() -> (batch_size, 32*16*16) = (100, 8192)\n",
    "        out = out.view(-1,55040)   #-1 will automatically update the batchsize as 100; 8192 flattens 32,16,16\n",
    "        #Then we forward through our fully connected layer \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    model = model.cuda()    \n",
    "loss_fn = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the CNN\n",
    "\n",
    "import time\n",
    "\n",
    "num_epochs = 150\n",
    "\n",
    "#Define the lists to store the results of loss and accuracy\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(num_epochs): \n",
    "    #Reset these below variables to 0 at the begining of every epoch\n",
    "    start = time.time()\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, data in enumerate(train_load):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "       # print(len(data['mfcc']))\n",
    "        for j in range(len(data['mfcc'])):\n",
    "            inputs = data['mfcc'][j]\n",
    "            inputs = torch.FloatTensor(np.expand_dims(inputs,axis=1))\n",
    "            #print(inputs.shape)\n",
    "            labels = Variable(data['labels'][j].view(1))\n",
    "            \n",
    "            \n",
    "            #print(\"labels\",labels)\n",
    "            # If we have GPU, shift the data to GPU\n",
    "            CUDA = torch.cuda.is_available()\n",
    "            if CUDA:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()            # Clear off the gradient in (w = w - gradient)\n",
    "            outputs = model(inputs) \n",
    "            #print(\"outputs\",outputs.shape,\"label\",labels.shape)\n",
    "            loss = loss_fn(outputs, labels)  \n",
    "            iter_loss += loss.item()       # Accumulate the loss\n",
    "            loss.backward()                 # Backpropagation \n",
    "            optimizer.step()                # Update the weights\n",
    "\n",
    "            # Record the correct predictions for training data \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum()\n",
    "            iterations += 1\n",
    "\n",
    "        # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(dataset)))\n",
    "\n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}'\n",
    "            .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1]))\n",
    "\n",
    "    #Testing\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "#     model.eval()                    # Put the network into evaluation mode\n",
    "    \n",
    "#     for i, (inputs, labels) in enumerate(test_load):\n",
    "        \n",
    "#         # Convert torch tensor to Variable\n",
    "#         inputs = Variable(inputs)\n",
    "#         labels = Variable(labels)\n",
    "        \n",
    "#         CUDA = torch.cuda.is_available()\n",
    "#         if CUDA:\n",
    "#             inputs = inputs.cuda()\n",
    "#             labels = labels.cuda()\n",
    "        \n",
    "#         outputs = model(inputs)     \n",
    "#         loss = loss_fn(outputs, labels) # Calculate the loss\n",
    "#         loss += loss.data[0]\n",
    "#         # Record the correct predictions for training data\n",
    "#         _, predicted = torch.max(outputs, 1)\n",
    "#         correct += (predicted == labels).sum()\n",
    "        \n",
    "#         iterations += 1\n",
    "\n",
    "#     # Record the Testing loss\n",
    "#     test_loss.append(loss/iterations)\n",
    "#     # Record the Testing accuracy\n",
    "#     test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "#     stop = time.time()\n",
    "    \n",
    "#     print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}, Time: {}s'\n",
    "#            .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], test_loss[-1], test_accuracy[-1], stop-start))\n",
    "\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
